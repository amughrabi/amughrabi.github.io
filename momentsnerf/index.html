
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MomentsNeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://amughrabi.github.io/prenerf/img/dish_1550704750.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://amughrabi.github.io/prenerf/"/>
    <meta property="og:title" content="MomentsNeRF: Enriching Unbounded Appearances for Neural Radiance Fields" />
    <meta property="og:description" content="Neural radiance fields (NeRF) appeared recently as a powerful tool to generate realistic views of objects and confined areas. Still, they face serious challenges with open scenes, where the camera has unrestricted movement and content can appear at any distance. In such scenarios, current NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow training times, and might display irregularities, because of the challenging task of reconstructing an extensive scene from a limited number of images. We propose a new framework to boost the performance of NeRF-based architectures yielding significantly superior outcomes compared to the prior work. Our solution overcomes several obstacles that plagued earlier versions of NeRF, including handling multiple video inputs, selecting keyframes, and extracting poses from real-world frames that are ambiguous and symmetrical. Furthermore, we applied our framework, dubbed as &quot;MomentsNeRF&quot;, to enable the use of the Nutrition5k dataset in NeRF and introduce an updated version of this dataset, known as the N5k360 dataset." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="MomentsNeRF: Enriching Unbounded Appearances for Neural Radiance Fields" />
    <meta name="twitter:description" content="Neural radiance fields (NeRF) appeared recently as a powerful tool to generate realistic views of objects and confined areas. Still, they face serious challenges with open scenes, where the camera has unrestricted movement and content can appear at any distance. In such scenarios, current NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow training times, and might display irregularities, because of the challenging task of reconstructing an extensive scene from a limited number of images. We propose a new framework to boost the performance of NeRF-based architectures yielding significantly superior outcomes compared to the prior work. Our solution overcomes several obstacles that plagued earlier versions of NeRF, including handling multiple video inputs, selecting keyframes, and extracting poses from real-world frames that are ambiguous and symmetrical. Furthermore, we applied our framework, dubbed as &quot;MomentsNeRF&quot;, to enable the use of the Nutrition5k dataset in NeRF and introduce an updated version of this dataset, known as the N5k360 dataset." />
    <meta name="twitter:image" content="/img/dish_1550704750.png" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <!-- Image slider   -->
    <script
            defer
            src="https://unpkg.com/img-comparison-slider@7/dist/index.js"
    ></script>
    <link
            rel="stylesheet"
            href="https://unpkg.com/img-comparison-slider@7/dist/styles.css"
    />
    <style>
        .before,
        .after {
            margin: 0;
        }

        .before figcaption,
        .after figcaption {
            background: #fff;
            border: 1px solid #c0c0c0;
            border-radius: 12px;
            color: #2e3452;
            opacity: 0.8;
            padding: 12px;
            position: absolute;
            top: 50%;
            transform: translateY(-50%);
            line-height: 100%;
        }

        .before figcaption {
            left: 12px;
        }

        .after figcaption {
            right: 12px;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>MomentsNeRF:</b> <br/>Incorporating Orthogonal Moments in Convolutional Neural Networks for One or Few-Shot Neural Rendering<br/>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://amughrabi.github.io/">
                          Ahmad AlMughrabi
                        </a>
                        </br>Universitat de Barcelona
                    </li>
                    <li>
                        <a href="https://ricjm.github.io/">
                            Ricardo Marques
                        </a>
                        </br>Universitat de Barcelona
                    </li>
                    <li>
                        <a href="http://www.ub.edu/cvub/petiaradeva/">
                            Petia Radeva
                        </a>
                        </br>Universitat de Barcelona
                    </li>
                </ul>
<!--                <small>-->
<!--                    * These authors contributed equally to this work.-->
<!--                </small>-->
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/tbd">
                            <image src="img/360_paper_image.jpg" height="60px"/>
                            <h4><strong>Paper <br/>(will be available soon.)</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://ubarcelona-my.sharepoint.com/:u:/g/personal/aalmugal14_alumnes_ub_edu/EVjjhdLjWRtDk0iwNEHuzMkBiEgZzBvbSgRG3a8TBXACbg?e=0v0GVX">
                            <image src="img/logo_1.png" height="60px"/>
                            <h4><strong>Pretrained models (1.2G) </strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/CVUBLab/moments-nerf">
                            <image src="img/github.png" height="60px"/>
                            <h4><strong>Code <br/>(will be available soon.)</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="pdf/MomentsNeRFPoster_new.pdf">
                            <image src="img/poster.jpg" height="60px"/>
                            <h4><strong>Poster <br/>(4.6M)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div style="font-size: 17px; text-align: center">
                    <div style="width: 33%; display: inline-block; position:relative;">
                        <img src="img/fig_sample_a.png" alt="a sample" width="100%"/>
                    </div>
                    <div style="width: 33%; display: inline-block; position:relative;">
                        <img src="img/fig_sample_b.png" alt="a sample" width="100%"/>
                    </div>
                    <div style="width: 33%; display: inline-block; position:relative;">
                        <img src="img/fig_sample_c.png" alt="a sample" width="100%"/>
                    </div>
                    <div style="width: 33%; display: inline-block; position:relative;">
                        <strong>PixelNeRF</strong> <br> <small>PSNR=25.624 SSIM=0.792 LPIPS=0.403 DISTS=0.235</small>
                    </div>
                    <div style="width: 33%; display: inline-block; position:relative;">
                        <strong>Ours</strong> <br> <small>PSNR=28.501 SSIM=0.83 LPIPS=0.336 DISTS=0.175</small>
                    </div>
                    <div style="width: 33%; display: inline-block; position:relative; vertical-align: top">
                        <strong>Reference</strong> <br>
                    </div>
                </div>

            </div>
            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    Qualitative comparison on DTU dataset. We show novel views rendered by PixelNeRF and MomentsNeRF compared to the reference in 3 views settings for scan114 scene.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="text-align: center">
                    Abstract
                </h3>
                <p class="text-justify">
                    We propose MomentsNeRF, one and a few-shot learning framework that predicts a neural representation of a scene using Orthogonal Moments. Our architecture offers a transfer learning method to pre-train on multi-scenes and incorporate a per-scene optimisation at test time using one or a few images. Our method leverages transfer learning which learns from features extracted from orthogonal Gabor and Zernike moments. Our approach shows a better performance in synthesising the scene details in terms of complex texture and shape capturing, noise reduction, artefact elimination, and completing the missing parts compared to the recent one and a few-shot neural rendering frameworks. We conduct extensive experiments on real scenes from the DTU dataset. MomentsNeRF improves the existing approaches by 3.39 dB PSNR, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS metrics. Moreover, MomentsNeRF achieves state of the art performance for both, novel view synthesis and single-image 3D view reconstruction.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img src="img/momentsnerf.png" alt="proposed method" width="100%"/>
                <p class="text-center">
                    Our proposed framework diagram, which outlines the entire workflow from a set of videos to any NeRF-like application. The data representation used in the diagram consists of four input videos that were taken from the Nutrition5k dataset
                </p>
            </div>
        </div>
        <style>
            .col-md-3 {
                width: 22%;
                /*padding: 0.25em;*/
            }
        </style>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="text-align: center">
                    Rendering
                </h3>
            </div>
        </div>

        <div class="row">
            <div class="col-md-4 col-md-offset-2">
                <img-comparison-slider>
                    <figure slot="first" class="before">
                        <img width="100%" src="img/pixelnerf/scan21/000015.png" />
                        <figcaption>PixelNeRF</figcaption>
                    </figure>
                    <figure slot="second" class="after">
                        <img width="100%" src="img/moments/scan21/000015.png" />
                        <figcaption>Ours</figcaption>
                    </figure>
                </img-comparison-slider>
                <div style="display:inline-block; position:relative; text-align: center; width: 100%; font-weight: bold">Scan21/1v</div>
            </div>
            <div class="col-md-4 col-md-offset-0">
                <img-comparison-slider>
                    <figure slot="first" class="before">
                        <img width="100%" src="img/pixelnerf/scan55/000022.png" />
                        <figcaption>PixelNeRF</figcaption>
                    </figure>
                    <figure slot="second" class="after">
                        <img width="100%" src="img/moments/scan55/000022.png" />
                        <figcaption>Ours</figcaption>
                    </figure>
                </img-comparison-slider>
                <div style="display:inline-block; position:relative; text-align: center; width: 100%; font-weight: bold">Scan55/3v</div>
            </div>
            <div class="col-md-4 col-md-offset-2">
                <img-comparison-slider>
                    <figure slot="first" class="before">
                        <img width="100%" src="img/pixelnerf/scan110/000022.png" />
                        <figcaption>PixelNeRF</figcaption>
                    </figure>
                    <figure slot="second" class="after">
                        <img width="100%" src="img/moments/scan110/000022.png" />
                        <figcaption>Ours</figcaption>
                    </figure>
                </img-comparison-slider>
                <div style="display:inline-block; position:relative; text-align: center; width: 100%; font-weight: bold">Scan110/6v</div>
            </div>
            <div class="col-md-4 col-md-offset-0">
                <img-comparison-slider>
                    <figure slot="first" class="before">
                        <img width="100%" src="img/pixelnerf/scan114/000033.png" />
                        <figcaption>PixelNeRF</figcaption>
                    </figure>
                    <figure slot="second" class="after">
                        <img width="100%" src="img/moments/scan114/000033.png" />
                        <figcaption>Ours</figcaption>
                    </figure>
                </img-comparison-slider>
                <div style="display:inline-block; position:relative; text-align: center; width: 100%; font-weight: bold">Scan114/9v</div>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    Rendered images from PixelNeRF and our model.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                        Will be available soon.
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://github.com/jonbarron/website">here</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
