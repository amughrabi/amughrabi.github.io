
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Pre-NeRF 360</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://amughrabi.github.io/prenerf/"/>
    <meta property="og:title" content="Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields" />
    <meta property="og:description" content="Neural radiance fields (NeRF) appeared recently as a powerful tool to generate realistic views of objects and confined areas. Still, they face serious challenges with open scenes, where the camera has unrestricted movement and content can appear at any distance. In such scenarios, current NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow training times, and might display irregularities, because of the challenging task of reconstructing an extensive scene from a limited number of images. We propose a new framework to boost the performance of NeRF-based architectures yielding significantly superior outcomes compared to the prior work. Our solution overcomes several obstacles that plagued earlier versions of NeRF, including handling multiple video inputs, selecting keyframes, and extracting poses from real-world frames that are ambiguous and symmetrical. Furthermore, we applied our framework, dubbed as &quot;Pre-NeRF 360&quot;, to enable the use of the Nutrition5k dataset in NeRF and introduce an updated version of this dataset, known as the N5k360 dataset." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields" />
    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Pre-NeRF 360:</b> <br/>Enriching Unbounded Appearances for Neural Radiance Fields<br/>
                <small>
								ICCV 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://amughrabi.github.io/">
                          Ahmad AlMughrabi
                        </a>
                        </br>Universitat de Barcelona
                    </li>
                    <li>
                        <a href="https://umairharon.github.io/">
                            Umair Haroon
                        </a>
                        </br>Universitat de Barcelona
                    </li>
                    <li>
                        <a href="http://www.ub.edu/cvub/petiaradeva/">
                            Petia Radeva
                        </a>
                        </br>Universitat de Barcelona
                    </li><br>
                    <li>
                        <a href="https://mat.ub.edu/departament/professors/marques-ricardo/">
                          Ricardo Marques
                        </a>
                        </br>Universitat de Barcelona
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/tbd">
                            <image src="img/360_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="tbd">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                        <li>
                            <a href="https://ubarcelona-my.sharepoint.com/:u:/g/personal/aalmugal14_alumnes_ub_edu/ETiRyyTXJL9DhiK8SkTG0sEBz0V-faGZ2SqKWA6478bRRw?e=ZFAR76" target="popup" onclick="window.open('https://ubarcelona-my.sharepoint.com/:u:/g/personal/aalmugal14_alumnes_ub_edu/ETiRyyTXJL9DhiK8SkTG0sEBz0V-faGZ2SqKWA6478bRRw?e=ZFAR76','popup','width=600,height=400')">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>N5k360 Dataset (214G)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://ubarcelona-my.sharepoint.com/:u:/g/personal/aalmugal14_alumnes_ub_edu/EVeiw2f8ruFBhXM0WNwJtJgBqxrU0jhy9OFrfVymHxVcAg?e=KTBPJB" target="popup" onclick="window.open('https://ubarcelona-my.sharepoint.com/:u:/g/personal/aalmugal14_alumnes_ub_edu/EVeiw2f8ruFBhXM0WNwJtJgBqxrU0jhy9OFrfVymHxVcAg?e=KTBPJB','popup','width=600,height=400')">
                                <image src="img/database_icon.png" height="60px">
                                    <h4><strong>Evaluation Dataset (1.18G)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/amughrabi/N5k360">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/gardenvase_720.mp4" type="video/mp4" />
                </video>
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
							Rendered images and depths from our model.
							</p>
						</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Neural radiance fields (NeRF) appeared recently as a powerful tool to generate realistic views of objects and confined areas. Still, they face serious challenges with open scenes, where the camera has unrestricted movement and content can appear at any distance. In such scenarios, current NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow training times, and might display irregularities, because of the challenging task of reconstructing an extensive scene from a limited number of images. We propose a new framework to boost the performance of NeRF-based architectures yielding significantly superior outcomes compared to the prior work. Our solution overcomes several obstacles that plagued earlier versions of NeRF, including handling multiple video inputs, selecting keyframes, and extracting poses from real-world frames that are ambiguous and symmetrical. Furthermore, we applied our framework, dubbed as "Pre-NeRF 360", to enable the use of the Nutrition5k dataset in NeRF and introduce an updated version of this dataset, known as the N5k360 dataset.
                </p>
            </div>
        </div>



       <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/zBSH-k9GbV4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>-->

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{barron2022mipnerf360,
    title={Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields},
    author={Jonathan T. Barron and Ben Mildenhall and 
            Dor Verbin and Pratul P. Srinivasan and Peter Hedman},
    journal={CVPR},
    year={2022}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://github.com/jonbarron/website">Jon Barron's public academic website</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
